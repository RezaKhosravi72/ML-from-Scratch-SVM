{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa3b17a9-3aea-440b-8925-04d4eef1d3a2",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM) from Scratch\n",
    "\n",
    "The code is implementing a linear Support Vector Machine (SVM) classifier to classify synthetic 2D data into two classes.\n",
    "\n",
    "### Key steps:\n",
    "\n",
    "1. Data is generated using make_blobs from scikit-learn with two classes. \n",
    "\n",
    "2. SVM class initializes weights (w) and bias (b) which define the decision boundary hyperplane.\n",
    "\n",
    "3. The fit() method calculates w and b using SGD to maximize the margin between classes. \n",
    "\n",
    "4. The predict() method returns class predictions based on the sign of w*X + b.\n",
    "\n",
    "5. visualize_svm() plots the data, decision boundary, and margins.\n",
    "\n",
    "### The main aspects demonstrated are:\n",
    "\n",
    "- Linear SVM finds optimal separating hyperplane to maximize margin\n",
    "- Optimization of w,b via SGD \n",
    "- Prediction based on hyperplane equation\n",
    "- Visualization of decision boundary and margins\n",
    "\n",
    "This provides an intuitive understanding of how SVM finds the optimal linear decision boundary between classes to maximize classification margins. The code implements the core math behind a linear SVM classifier.\n",
    "\n",
    "The only library used is scikit-learn for data generation, the SVM logic is implemented from scratch to better understand the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6251329b-04c3-4218-a164-72bd1f3ee620",
   "metadata": {},
   "source": [
    "### The explanation of the fit method:\n",
    "\n",
    "The fit method initializes the model parameters, then iterates through samples to update w and b to maximize the margin between classes, for a number of epochs defined by n_iter.\n",
    "\n",
    "def fit(self, X, y):\n",
    "\n",
    "This is the header for the fit method, which takes in the features X and labels y.\n",
    "\n",
    "N, n_features = X.shape\n",
    "\n",
    "Gets the number of samples N and number of features n_features from the shape of X. \n",
    "\n",
    "y = np.where(y<=0, -1, 1)\n",
    "\n",
    "Separates the labels y into -1 and 1 for the two classes.\n",
    "\n",
    "self.w = np.zeros(n_features) \n",
    "\n",
    "Initializes the weights vector w to zeros, of size number of features.\n",
    "\n",
    "self.b = 0\n",
    "\n",
    "Initializes the bias b to 0.\n",
    "\n",
    "for _ in range(self.n_iter):\n",
    "\n",
    "Starts a for loop to iterate the training for n_iter number of times.\n",
    "\n",
    "for idx, x_idx in enumerate(X):\n",
    "\n",
    "Loops through each sample x_idx in X.\n",
    "\n",
    "condition = y[idx] * (np.dot(x_idx, self.w) - self.b) >= 1\n",
    "\n",
    "Checks if sample is on correct side of margin based on sign of dot product. \n",
    "\n",
    "if condition:\n",
    "\n",
    "If sample is on correct side, update just weights.\n",
    "\n",
    "self.w -= self.lr * (2 * self.lambda_param * self.w)\n",
    "\n",
    "else:\n",
    "\n",
    "If on wrong side, update both weights and bias. \n",
    "\n",
    "self.w -= self.lr * ( (2 * self.lambda_param * self.w) + ((1/N) * np.dot(y[idx], x_idx)) )\n",
    "self.b -= self.lr * y[idx]\n",
    "\n",
    "This implements the SGD update step to learn the optimal w and b.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
